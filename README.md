# Emergem
"Emerge-a-gem"
A live creation environment that holds an Emernet (Emergence-Network) of Nodes (Generic Signal Processing Units)
Provides an interface for managing nodes, adjusting node parameters, emerging nodes together, capturing nodes, and composing a song/video out of captures

Designed by observing the music within (the life experience), creating a visual-systems reflection of it, and basing the Emergem's design off that reflection

# Sketches from the past for reference 

## Problem
![image](https://github.com/user-attachments/assets/66c6304d-7ce6-49ec-b264-ee6dc0b8b8df)

## Intuitive Sound Design
![image](https://github.com/user-attachments/assets/2e947eab-70d3-4ad8-9166-3e35feac150d)

## Expression Factor x Virtuosity Graph
![image](https://github.com/user-attachments/assets/b6b46697-77e4-4290-9a08-5b7003db1257)

## Comparison of Usefulness/Capabilities
![image](https://github.com/user-attachments/assets/3d6b261c-2998-4dfc-ad32-cfb59d54427d)
- Emergem is more capable then DAW due to having a generic node graph (free to connect from any unit node output to compatible unit node input). It also has generic polyphonic output/input types for nodes (e.g. audio, control signals, midi signals, gate signals, pitch signals, phase signals, signals for creating visuals...)
- Emergem also provides interface for composing music.. comes up with a new way to record parts of or an entire generic graph, as well as create sections, and sequence them to create a song.. through an interface that is like a feature extended live-looping interface (so it is designed for live creation). (See Kokopelli-Interfaces Vcvrack module "Circle" for an idea)

## Emernet and relationships in it
![Untitled](https://github.com/user-attachments/assets/6a06e72a-4344-4002-a391-24255d6ed065)

## Design Principle
        - Rooted in the emergence of orthogonal components
		- "Sound design via connecting signal processing nodes together, all which do their own one thing"
	- Musician focused sound creation, not technology focused
		○ Functions intuitively mapping to functions in the experiential sound space
	- Uses sound language.
		○ E.g. Voltage Gain -> Strength, Delay Feedback -> Echo Decay, Ring Modulator -> ??
		○ Uses synesthesia / visual language.
	- Interacting with a real-time, 'living' system.
 	- Feels like using an instrument, not a workstation 
  		- (Use touchscreen to control blocks, or use custom physical Emergem Controller)
	- One can develop with it as a sound designer/composer
		○ "I can recreate this sound in my head by decomposing it into key characteristics such as: Pitch around C5, Timbre is Nasaly, High Volume, Positioned far in a room and to the left, Dynamics are clipping, Composed of Grains, Fuzzy. This is the language of the nodes in the interface."


### Example Control Units
![image](https://github.com/user-attachments/assets/5c892553-6483-4c45-b718-dbdcec762055)

## "As Above, So Below" Manifestation Model
![image](https://github.com/user-attachments/assets/10e42500-c49a-4652-8fd8-5ad019ee492f)






